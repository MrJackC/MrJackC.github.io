import{_ as a,c as o,a as e,o as n}from"./app-BOcQBfH9.js";const i={};function l(r,s){return n(),o("div",null,s[0]||(s[0]=[e(`<h1 id="线上oom-记一次oom排查过程" tabindex="-1"><a class="header-anchor" href="#线上oom-记一次oom排查过程"><span>线上OOM-记一次OOM排查过程</span></a></h1><h2 id="_1-背景" tabindex="-1"><a class="header-anchor" href="#_1-背景"><span>1. 背景</span></a></h2><p>项目中有个需求是将爬虫爬取到的网页数据（存放在mongodb）, 做数据清理后放入搜索引擎（solr）中。总共350w的网页数据，如果按正常速度同步10个小时即可完成。但我们实际测试发现，随着时间推移，同步时间越来越长，挂了一天只同步了100w数据。且后面越来越慢。领导找到我，让我帮忙排查解决</p><h2 id="_2-解决一-mongodb-大数据量分页查询效率问题" tabindex="-1"><a class="header-anchor" href="#_2-解决一-mongodb-大数据量分页查询效率问题"><span>2. 解决一: mongodb 大数据量分页查询效率问题</span></a></h2><p>通过查阅资料了解到</p><p>虽然MongoDB提供了skip()和limit()方法。看起来，分页已经实现了，但是官方文档并不推荐，说会扫描全部文档，然后再返回结果。</p><blockquote><p>The cursor.skip() method requires the server to scan from the beginning of the input results set before beginning to return results. As the offset increases, cursor.skip() will become slower.</p><p>cursor.skip() 方法要求服务器先从输入结果集开始扫描，然后再开始返回结果。随着偏移量的增加，cursor.skip() 会变慢。</p></blockquote><p>所以，需要一种更快的方式。其实和mysql数量大之后不推荐用limit m,n一样，解决方案是先查出当前页的第一条，然后顺序数pageSize条。MongoDB官方也是这样推荐的。</p><h3 id="_2-1-解决方案1-通过-id-比较取分页" tabindex="-1"><a class="header-anchor" href="#_2-1-解决方案1-通过-id-比较取分页"><span>2.1 解决方案1：通过_id 比较取分页</span></a></h3><p>我们假设基于_id的条件进行查询比较。事实上，这个比较的基准字段可以是任何你想要的有序的字段，比如时间戳。</p><div class="language-bash" data-ext="bash" data-title="bash"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">//Page</span><span style="color:#D19A66;--shiki-dark:#D19A66;"> 1</span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">db.users.find</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">().limit(</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">pageSize</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">);</span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">//Find</span><span style="color:#98C379;--shiki-dark:#98C379;"> the</span><span style="color:#98C379;--shiki-dark:#98C379;"> id</span><span style="color:#98C379;--shiki-dark:#98C379;"> of</span><span style="color:#98C379;--shiki-dark:#98C379;"> the</span><span style="color:#98C379;--shiki-dark:#98C379;"> last</span><span style="color:#98C379;--shiki-dark:#98C379;"> document</span><span style="color:#98C379;--shiki-dark:#98C379;"> in</span><span style="color:#98C379;--shiki-dark:#98C379;"> this</span><span style="color:#98C379;--shiki-dark:#98C379;"> page</span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">last_id</span><span style="color:#98C379;--shiki-dark:#98C379;"> =</span><span style="color:#98C379;--shiki-dark:#98C379;"> ...</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">//Page</span><span style="color:#D19A66;--shiki-dark:#D19A66;"> 2</span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">users</span><span style="color:#98C379;--shiki-dark:#98C379;"> =</span><span style="color:#98C379;--shiki-dark:#98C379;"> db.users.find</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">({</span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">  &#39;_id&#39;</span><span style="color:#98C379;--shiki-dark:#98C379;"> :{</span><span style="color:#98C379;--shiki-dark:#98C379;"> &quot;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">$gt</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;</span><span style="color:#98C379;--shiki-dark:#98C379;"> :ObjectId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">&quot;5b16c194666cd10add402c87&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)</span><span style="color:#98C379;--shiki-dark:#98C379;">}</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">}).limit(10)</span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">//Update</span><span style="color:#98C379;--shiki-dark:#98C379;"> the</span><span style="color:#98C379;--shiki-dark:#98C379;"> last</span><span style="color:#98C379;--shiki-dark:#98C379;"> id</span><span style="color:#98C379;--shiki-dark:#98C379;"> with</span><span style="color:#98C379;--shiki-dark:#98C379;"> the</span><span style="color:#98C379;--shiki-dark:#98C379;"> id</span><span style="color:#98C379;--shiki-dark:#98C379;"> of</span><span style="color:#98C379;--shiki-dark:#98C379;"> the</span><span style="color:#98C379;--shiki-dark:#98C379;"> last</span><span style="color:#98C379;--shiki-dark:#98C379;"> document</span><span style="color:#98C379;--shiki-dark:#98C379;"> in</span><span style="color:#98C379;--shiki-dark:#98C379;"> this</span><span style="color:#98C379;--shiki-dark:#98C379;"> page</span></span>
<span class="line"><span style="color:#61AFEF;--shiki-dark:#61AFEF;">last_id</span><span style="color:#98C379;--shiki-dark:#98C379;"> =</span><span style="color:#98C379;--shiki-dark:#98C379;"> ...</span></span></code></pre></div><p>显然，第一页和后面的不同。对于构建分页API, 我们可以要求用户必须传递pageSize, lastId。</p><ul><li>pageSize 页面大小</li><li>lastId 上一页的最后一条记录的id，如果不传，则将强制为第一页</li></ul><h3 id="_2-2-解决方案2-通过游标来查询" tabindex="-1"><a class="header-anchor" href="#_2-2-解决方案2-通过游标来查询"><span>2.2 解决方案2：通过游标来查询</span></a></h3><div class="language-java" data-ext="java" data-title="java"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E5C07B;--shiki-dark:#E5C07B;">        FindIterable</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&lt;</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Document</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> findIterable </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> mongoTemplate</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">getCollection</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">mongoTemplate</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">getCollectionName</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(tClass))</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                .</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">find</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">()</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                .</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">noCursorTimeout</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#D19A66;--shiki-dark:#D19A66;">true</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                .</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">batchSize</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#D19A66;--shiki-dark:#D19A66;">1000</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">);</span></span>
<span class="line"></span>
<span class="line"><span style="color:#E5C07B;--shiki-dark:#E5C07B;">        MongoCursor</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&lt;</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Document</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> cursor </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> findIterable</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">cursor</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">();</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">        while</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> (</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">cursor</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;">hasNext</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">()</span><span style="color:#E06C75;--shiki-dark:#E06C75;">){</span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">        </span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">        }</span></span></code></pre></div><h3 id="_2-3-优化成果" tabindex="-1"><a class="header-anchor" href="#_2-3-优化成果"><span>2.3 优化成果</span></a></h3><p>最终我采用游标的方式来查询，在做数据清理的时候，非常稳定，不会随着深度增加而越来越慢，花费9小时左右完成同步</p><h2 id="_3-oom引发-通过多线程来优化" tabindex="-1"><a class="header-anchor" href="#_3-oom引发-通过多线程来优化"><span>3. OOM引发：通过多线程来优化</span></a></h2><p>花费9个小时还是太久了，大部分时间都浪费在数据清理和上传到solr 上。我们希望通过多线程来优化</p><p>但改成多线程版本后发现OOM 了</p><h3 id="_3-1-oom-gc-overhead-limit-exceeded" tabindex="-1"><a class="header-anchor" href="#_3-1-oom-gc-overhead-limit-exceeded"><span>3.1 OOM: GC overhead limit exceeded</span></a></h3><p>我们知道 OOM: GC overhead limit exceeded ，意味着超过98%的时间用来做GC并且回收了不到2%的堆内存</p><blockquote><p>并行/并发回收器在GC回收时间过长时会抛出OutOfMemroyError。过长的定义是，超过98%的时间用来做GC并且回收了不到2%的堆内存。用来避免内存过小造成应用不能正常工作。</p></blockquote><p>我们查看gc日志分析，后期gc特别频繁</p><figure><img src="https://cdn.jsdelivr.net/gh/MrJackC/PicGoImages/other/202403131410551.png" alt="image-20220729133633739" tabindex="0" loading="lazy"><figcaption>image-20220729133633739</figcaption></figure><h3 id="_3-2-查看-gc-roots-引用链" tabindex="-1"><a class="header-anchor" href="#_3-2-查看-gc-roots-引用链"><span>3.2 查看 GC-Roots 引用链</span></a></h3><p>我们知道OOM 堆内存溢出，主要因为 Java 堆中不断的创建对象，并且 <code>GC-Roots</code> 到对象之间存在引用链，这样 <code>JVM</code> 就不会回收对象。才导致内存溢出</p><p>我们查看 GC-Roots 引用链 ，查看对象和 <code>GC-Roots</code> 是如何进行关联的，是否存在对象的生命周期过长等问题</p><p>我们使用JProfiler 可以看到 堆中存在大量我们爬取的网页内容，并且远超的我们的堆内存范围</p><figure><img src="https://cdn.jsdelivr.net/gh/MrJackC/PicGoImages/other/202403131410604.png" alt="image-20220827140237927" tabindex="0" loading="lazy"><figcaption>image-20220827140237927</figcaption></figure><h3 id="_3-3-分析我们的代码-查找原因" tabindex="-1"><a class="header-anchor" href="#_3-3-分析我们的代码-查找原因"><span>3.3 分析我们的代码，查找原因</span></a></h3><p>通过gcroot 我们已经知道是的对象无法回收。</p><p>我们350w的网页内容直接放到堆中处理，肯定会存在OOM。但我们使用了线程池，线程池中带有阻塞队列，按理应该会阻塞才对。消费完才能再生产，现在不生效肯定是线程池的问题</p><p>通过排查发现，我们用spring bean 引的全局线程池，他的阻塞队列并没有设置拒绝策略，采用了默认的拒绝策略</p><div class="language-java" data-ext="java" data-title="java"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">// 构建一个10核心线程，20最大线程，最大队列为1000</span></span>
<span class="line"><span style="color:#E5C07B;--shiki-dark:#E5C07B;">ThreadPoolExecutor</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> executor </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#C678DD;--shiki-dark:#C678DD;"> new</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;"> ThreadPoolExecutor</span><span style="color:#E06C75;--shiki-dark:#E06C75;">(</span><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">,</span><span style="color:#D19A66;--shiki-dark:#D19A66;"> 20</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">,</span><span style="color:#D19A66;--shiki-dark:#D19A66;"> 200</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">,</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> TimeUnit</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">MILLISECONDS</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">,</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">        new</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> ArrayBlockingQueue</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&lt;</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Runnable</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">(</span><span style="color:#D19A66;--shiki-dark:#D19A66;">1000</span><span style="color:#E06C75;--shiki-dark:#E06C75;">))</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">;</span></span></code></pre></div><blockquote><p>默认的拒绝策略是ThreadPoolExecutor.AbortPolicy:丢弃任务并抛出RejectedExecutionException异常</p></blockquote><p>我们加上了拒绝策略,<strong>ThreadPoolExecutor.CallerRunsPolicy：由调用线程处理该任务 。</strong></p><blockquote><p>ps: <strong>加了CallerRunsPolicy 阻塞队列才能发挥阻塞作用</strong>。</p></blockquote><h3 id="_3-4-优化成功" tabindex="-1"><a class="header-anchor" href="#_3-4-优化成功"><span>3.4 优化成功</span></a></h3><p>我们加入拒绝策略后，阻塞队列产生了效果。产生和消费处于平衡状态，生产一批，消费一批。内存稳定。最终花费3小时完成了数据清理工作</p><h2 id="参考文章" tabindex="-1"><a class="header-anchor" href="#参考文章"><span>参考文章</span></a></h2><p><a href="https://www.cnblogs.com/woshimrf/p/mongodb-pagenation-performance.html" target="_blank" rel="noopener noreferrer">MongoDB分页的Java实现和分页需求的思考</a></p>`,42)]))}const t=a(i,[["render",l],["__file","online-oom-thread-oom2.html.vue"]]),c=JSON.parse('{"path":"/posts/Daily-Thoughts/deepImpression/online-oom-thread-oom2.html","title":"线上OOM-记一次OOM排查过程","lang":"zh-CN","frontmatter":{"aliases":"线上OOM-记一次OOM排查过程","tags":null,"cssclass":null,"source":null,"order":120,"category":["OOM"],"created":"2024-02-22 10:53","updated":"2024-03-13 14:10","description":"线上OOM-记一次OOM排查过程 1. 背景 项目中有个需求是将爬虫爬取到的网页数据（存放在mongodb）, 做数据清理后放入搜索引擎（solr）中。总共350w的网页数据，如果按正常速度同步10个小时即可完成。但我们实际测试发现，随着时间推移，同步时间越来越长，挂了一天只同步了100w数据。且后面越来越慢。领导找到我，让我帮忙排查解决 2. 解决一...","head":[["meta",{"property":"og:url","content":"https://mrjackc.github.io/posts/Daily-Thoughts/deepImpression/online-oom-thread-oom2.html"}],["meta",{"property":"og:site_name","content":"mrjason’s Blog"}],["meta",{"property":"og:title","content":"线上OOM-记一次OOM排查过程"}],["meta",{"property":"og:description","content":"线上OOM-记一次OOM排查过程 1. 背景 项目中有个需求是将爬虫爬取到的网页数据（存放在mongodb）, 做数据清理后放入搜索引擎（solr）中。总共350w的网页数据，如果按正常速度同步10个小时即可完成。但我们实际测试发现，随着时间推移，同步时间越来越长，挂了一天只同步了100w数据。且后面越来越慢。领导找到我，让我帮忙排查解决 2. 解决一..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://cdn.jsdelivr.net/gh/MrJackC/PicGoImages/other/202403131410551.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-10-28T01:58:08.000Z"}],["meta",{"property":"article:author","content":"MrJason"}],["meta",{"property":"article:modified_time","content":"2024-10-28T01:58:08.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"线上OOM-记一次OOM排查过程\\",\\"image\\":[\\"https://cdn.jsdelivr.net/gh/MrJackC/PicGoImages/other/202403131410551.png\\",\\"https://cdn.jsdelivr.net/gh/MrJackC/PicGoImages/other/202403131410604.png\\"],\\"dateModified\\":\\"2024-10-28T01:58:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"MrJason\\",\\"url\\":\\"https://mrjackc.github.io\\"}]}"]]},"headers":[{"level":2,"title":"1. 背景","slug":"_1-背景","link":"#_1-背景","children":[]},{"level":2,"title":"2. 解决一: mongodb 大数据量分页查询效率问题","slug":"_2-解决一-mongodb-大数据量分页查询效率问题","link":"#_2-解决一-mongodb-大数据量分页查询效率问题","children":[{"level":3,"title":"2.1 解决方案1：通过_id 比较取分页","slug":"_2-1-解决方案1-通过-id-比较取分页","link":"#_2-1-解决方案1-通过-id-比较取分页","children":[]},{"level":3,"title":"2.2 解决方案2：通过游标来查询","slug":"_2-2-解决方案2-通过游标来查询","link":"#_2-2-解决方案2-通过游标来查询","children":[]},{"level":3,"title":"2.3 优化成果","slug":"_2-3-优化成果","link":"#_2-3-优化成果","children":[]}]},{"level":2,"title":"3. OOM引发：通过多线程来优化","slug":"_3-oom引发-通过多线程来优化","link":"#_3-oom引发-通过多线程来优化","children":[{"level":3,"title":"3.1 OOM: GC overhead limit exceeded","slug":"_3-1-oom-gc-overhead-limit-exceeded","link":"#_3-1-oom-gc-overhead-limit-exceeded","children":[]},{"level":3,"title":"3.2 查看 GC-Roots 引用链","slug":"_3-2-查看-gc-roots-引用链","link":"#_3-2-查看-gc-roots-引用链","children":[]},{"level":3,"title":"3.3 分析我们的代码，查找原因","slug":"_3-3-分析我们的代码-查找原因","link":"#_3-3-分析我们的代码-查找原因","children":[]},{"level":3,"title":"3.4 优化成功","slug":"_3-4-优化成功","link":"#_3-4-优化成功","children":[]}]},{"level":2,"title":"参考文章","slug":"参考文章","link":"#参考文章","children":[]}],"git":{"createdTime":1730080688000,"updatedTime":1730080688000,"contributors":[{"name":"MrJason","email":"845886914@qq.com","commits":1}]},"readingTime":{"minutes":4.24,"words":1272},"filePathRelative":"posts/Daily-Thoughts/deepImpression/online-oom-thread-oom2.md","localizedDate":"2024年10月28日","excerpt":"\\n<h2>1. 背景</h2>\\n<p>项目中有个需求是将爬虫爬取到的网页数据（存放在mongodb）, 做数据清理后放入搜索引擎（solr）中。总共350w的网页数据，如果按正常速度同步10个小时即可完成。但我们实际测试发现，随着时间推移，同步时间越来越长，挂了一天只同步了100w数据。且后面越来越慢。领导找到我，让我帮忙排查解决</p>\\n<h2>2. 解决一: mongodb 大数据量分页查询效率问题</h2>\\n<p>通过查阅资料了解到</p>\\n<p>虽然MongoDB提供了skip()和limit()方法。看起来，分页已经实现了，但是官方文档并不推荐，说会扫描全部文档，然后再返回结果。</p>","autoDesc":true}');export{t as comp,c as data};
