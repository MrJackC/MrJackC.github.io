import{_ as a,c as n,a as l,o as i}from"./app-tJW29Kmg.js";const o={};function r(p,s){return i(),n("div",null,s[0]||(s[0]=[l(`<h1 id="spark环境的安装" tabindex="-1"><a class="header-anchor" href="#spark环境的安装"><span>Spark环境的安装</span></a></h1><hr><h2 id="一、-spark简介" tabindex="-1"><a class="header-anchor" href="#一、-spark简介"><span>一、 Spark简介</span></a></h2><h3 id="_1-1-spark是什么" tabindex="-1"><a class="header-anchor" href="#_1-1-spark是什么"><span>1.1 Spark是什么</span></a></h3><blockquote><p>是一种基于<strong>内存</strong>的快速、通用、可拓展的大数据<strong>分析计算引擎</strong>。</p></blockquote><h3 id="_1-2-hadoop-和-spark关联" tabindex="-1"><a class="header-anchor" href="#_1-2-hadoop-和-spark关联"><span>1.2 Hadoop 和 Spark关联</span></a></h3><blockquote><ol><li>hadoop ：2013年10月发布2.X (Yarn)版本；</li><li>spark ： 2013年6月，Spark成为了Apache基金会下的项目。</li><li>Spark可以理解为hadoop MR的升级版。</li></ol></blockquote><h4 id="_1-2-1-hadoop发展历史" tabindex="-1"><a class="header-anchor" href="#_1-2-1-hadoop发展历史"><span>1.2.1 hadoop发展历史</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 1.X 版本 --2011年发布</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">从架构的角度存在很多的问题</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. Namenode 是单点操作，所以容易出现单点故障，制约了HDFS的发展</span></span>
<span class="line"></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. Namenode的内存限制也影响了HDFS的发展</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">3</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后将结果写入物理存储设备。数据更多面临的是一次性计算，所以初衷是单一数据计算，不支持迭代计算</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. 资源调度和任务调度耦合在一起，无法扩展，</span><span style="color:#D19A66;--shiki-dark:#D19A66;">所以Hadoop1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">X版本只支持MR计算框架</span></span></code></pre></div><p>![image-20200602020855518](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602020855.png)</p><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 2.X 版本（Yarn） --2013.10月发布</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. </span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.X版本支持Namenode高可用</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. </span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.X版本使用新的资源调度框架Yarn，只做资源调度，不进行任务调度。</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">3</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. MR框架只做任务调度，可插拔，所以扩展性非常的强</span></span></code></pre></div><p>![image-20200602021032775](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602021032.png)</p><h4 id="_1-2-2-spark-技术" tabindex="-1"><a class="header-anchor" href="#_1-2-2-spark-技术"><span>1.2.2 Spark 技术</span></a></h4><blockquote><ol><li><p>Spark其实核心思想就是基于MR的，优化了MR数据处理的中间过程，提升了数据处理的性能</p></li><li><p>MR：多任务之间的数据会进行落盘</p></li><li><p>Spark：多任务之间的数据会在内存中。</p></li><li><p>因为内存大小也有上限的，当内存不足时，就会出现job运行失败，所以Spark并不是完全替代MR。</p></li></ol></blockquote><ul><li>MR和Spark区别</li></ul><p>![image-20200602021626723](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602021626.png)</p><p>![image-20200602021747457](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602021747.png)</p><ul><li>Spark的特点</li></ul><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. spark计算模型更加丰富，MR只有mapper和reducer， spark的计算模型模糊了mapper和reduce的界限，更容易使用;</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. spark使用scala语言开发，支持函数式编程，所以就更利用迭代式计算.</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">3</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. spark也有自己的任务调度器和资源调度器。</span></span></code></pre></div><p>![image-20200602022304372](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602022304.png)</p><h4 id="_1-2-3-spark-on-yarn" tabindex="-1"><a class="header-anchor" href="#_1-2-3-spark-on-yarn"><span>1.2.3 Spark On Yarn</span></a></h4><blockquote><p>在实际开发中，hadoop和Spark合二为一。</p><p>调度器：Hadoop的Yarn</p><p>任务执行：Spark的任务调度，Driver和Executor</p></blockquote><p>![image-20200602022554760](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602022554.png)</p><h3 id="_1-3-spark的核心框架" tabindex="-1"><a class="header-anchor" href="#_1-3-spark的核心框架"><span>1.3 Spark的核心框架</span></a></h3><p>![image-20200602181641619](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602181641.png)</p><blockquote><ol><li>Apache Spark Core : 最基础和最核心的功能</li><li>Spark SQL :用于处理关系型数据库</li><li>Spark Streaming：针对实时数据的处理流式计算的框架，Flink框架更有优势</li><li>Spark MLlib: 机器学习</li><li>Spark Graphx:面向图形计算</li></ol></blockquote><p>我们重点学习Spark前面三个框架。</p><h2 id="二、spark快速上手" tabindex="-1"><a class="header-anchor" href="#二、spark快速上手"><span>二、Spark快速上手</span></a></h2><h3 id="创建maven工程" tabindex="-1"><a class="header-anchor" href="#创建maven工程"><span>创建Maven工程</span></a></h3><h3 id="_2-1-增加scala插件" tabindex="-1"><a class="header-anchor" href="#_2-1-增加scala插件"><span>2.1 增加Scala插件</span></a></h3><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--当前使用的Spark版本为2.4.5，默认采用的Scala版本为2.12</span></span></code></pre></div><h3 id="_2-2-增加依赖关系" tabindex="-1"><a class="header-anchor" href="#_2-2-增加依赖关系"><span>2.2 增加依赖关系</span></a></h3><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--修改Maven项目中的POM文件，增加Spark框架的依赖关系。当前文件是基于Spark2.4.5版本，使用时请注意对应版本</span></span></code></pre></div><div class="language-xml line-numbers-mode" data-ext="xml" data-title="xml"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">dependencies</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">dependency</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">        &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">groupId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;org.apache.spark&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">groupId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">        &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">artifactId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;spark-core_2.12&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">artifactId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">        &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">version</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;2.4.5&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">version</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">dependency</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">dependencies</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">build</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">plugins</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">        &lt;!-- 该插件用于将Scala代码编译成class文件 --&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">        &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">plugin</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">groupId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;net.alchim31.maven&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">groupId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">artifactId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;scala-maven-plugin&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">artifactId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">version</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;3.2.2&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">version</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">executions</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">execution</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">                    &lt;!-- 声明绑定到maven的compile阶段 --&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                    &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goals</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                        &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goal</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;testCompile&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goal</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                    &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goals</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">execution</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">executions</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">        &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">plugin</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">        &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">plugin</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">groupId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;org.apache.maven.plugins&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">groupId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">artifactId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;maven-assembly-plugin&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">artifactId</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">version</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;3.0.0&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">version</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">configuration</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">descriptorRefs</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                    &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">descriptorRef</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;jar-with-dependencies&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">descriptorRef</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">descriptorRefs</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">configuration</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">executions</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">execution</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                    &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">id</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;make-assembly&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">id</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                    &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">phase</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;package&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">phase</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                    &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goals</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                        &lt;</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goal</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;single&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goal</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                    &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">goals</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">                &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">execution</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">            &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">executions</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">        &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">plugin</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    &lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">plugins</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&lt;/</span><span style="color:#E06C75;--shiki-dark:#E06C75;">build</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">&gt;</span></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-3-增加日志文件配置文件" tabindex="-1"><a class="header-anchor" href="#_2-3-增加日志文件配置文件"><span>2.3 增加日志文件配置文件</span></a></h3><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 在项目的resources目录中创建log4j.properties文件，并将如下信息添加到文件中（日志信息）：</span></span></code></pre></div><div class="language-properties line-numbers-mode" data-ext="properties" data-title="properties"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.rootCategory</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR, console</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.appender.console</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">org.apache.log4j.ConsoleAppender</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.appender.console.target</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">System.err</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.appender.console.layout</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">org.apache.log4j.PatternLayout</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.appender.console.layout.ConversionPattern</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Set the default spark-shell log level to ERROR. When running the spark-shell, the</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># log level for this class is used to overwrite the root logger&#39;s log level, so that</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># the user can have different defaults for the shell and regular Spark apps.</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.logger.org.apache.spark.repl.Main</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Settings to quiet third party logs that are too verbose</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.logger.org.spark_project.jetty</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.logger.org.spark_project.jetty.util.component.AbstractLifeCycle</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">log4j.logger.org.apache.spark.repl.SparkIMain$</span><span style="color:#C678DD;--shiki-dark:#C678DD;">exprTyper</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">log4j.logger.org.apache.spark.repl.SparkILoop$</span><span style="color:#C678DD;--shiki-dark:#C678DD;">SparkILoopInterpreter</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.logger.org.apache.parquet</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.logger.parquet</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">FATAL</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">ERROR</span></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_2-4-wordcount" tabindex="-1"><a class="header-anchor" href="#_2-4-wordcount"><span>2.4 WordCount</span></a></h3><ul><li>方法1：</li></ul><div class="language-scala" data-ext="scala" data-title="scala"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">步骤：</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//1. 创建Spark的环境</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//2. 连接Spark</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//3. 具体的操作</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//4. 关闭连接</span></span></code></pre></div><div class="language-scala line-numbers-mode" data-ext="scala" data-title="scala"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">object</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> Spark_WordCount</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> {</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">  def</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;"> main</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">args</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Array</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">]): </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Unit</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> =</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> {</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //需求：读取一个文件中的数据，求(word,count)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //1. 创建Spark的环境</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> sparkConf</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">SparkConf</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> =</span><span style="color:#C678DD;--shiki-dark:#C678DD;"> new</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> SparkConf</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">().setMaster(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;local&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">).setAppName(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;wordcount&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //2. 连接Spark</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> sc</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> =</span><span style="color:#C678DD;--shiki-dark:#C678DD;"> new</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> SparkContext</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(sparkConf)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //3. 具体的操作</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //3.1 读取数据</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> str</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> sc.textFile(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;input&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //3.2 扁平化数据</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> words</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> str.flatMap(_.split(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot; &quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //3.3 分组</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> wordtocount</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[(</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">, </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Iterable</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">])] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> words.groupBy(word </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=&gt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> word)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //3.4 结构化处理</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> wordcount</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[(</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">, </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Int</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> wordtocount.map {</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">      case</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> (word, iter) </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=&gt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> (word, iter.size)</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //3.5 数据采集并打印在控制台</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> result</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Array</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[(</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">, </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Int</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> wordcount.collect()</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    println(result.mkString(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;,&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //4. 关闭连接</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    sc.stop()</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">  }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>方法二</li></ul><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 在方法1的基础上，将分组和结构化处理操作进行优化。</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 方法1：//3.3 分组 +  //3.4 结构化处理</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">分组操作结果：(spark , List(spark,spark)) </span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">结构化处理结果：(spark,</span><span style="color:#D19A66;--shiki-dark:#D19A66;">list</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">length</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 优化方法：在进行分组操作的同时就进行单词出现的次数进行计算。</span></span></code></pre></div><div class="language-scala line-numbers-mode" data-ext="scala" data-title="scala"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">object</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> Spark_WordCount1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> {</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">  def</span><span style="color:#61AFEF;--shiki-dark:#61AFEF;"> main</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#E06C75;font-style:italic;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">args</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Array</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">]): </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Unit</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> =</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> {</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //1. 创建spark的环境</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> sparkConf</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">SparkConf</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> =</span><span style="color:#C678DD;--shiki-dark:#C678DD;"> new</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> SparkConf</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">().setMaster(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;local&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">).setAppName(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;wordcount&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //2. 与spark进行连接</span></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> sc</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> =</span><span style="color:#C678DD;--shiki-dark:#C678DD;"> new</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;"> SparkContext</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(sparkConf)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //3. 具体的操作</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> datas</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> sc.textFile(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;input&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> words</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> datas.flatMap(_.split(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot; &quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> wordto</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[(</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">, </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Int</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> words.map((_,</span><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"></span>
<span class="line"><span style="color:#C678DD;--shiki-dark:#C678DD;">    val</span><span style="color:#E06C75;--shiki-dark:#E06C75;"> wordcount</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">: </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">RDD</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[(</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">String</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">, </span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">Int</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)] </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> wordto.reduceByKey(_ </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">+</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> _)</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    println(wordcount.collect().mkString(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;,&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">))</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">    //4. 关闭连接</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    sc.stop()</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">    </span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">  }</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">}</span></span></code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="三、-spark运行环境" tabindex="-1"><a class="header-anchor" href="#三、-spark运行环境"><span>三、 Spark运行环境</span></a></h2><h3 id="_3-1-spark的运行环境" tabindex="-1"><a class="header-anchor" href="#_3-1-spark的运行环境"><span>3.1 Spark的运行环境</span></a></h3><p>![image-20200602185736933](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602185736.png)</p><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">我们主要学习3种环境，并简单讲述一下2种环境</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">学习的环境：</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. 本地运行模式：</span><span style="color:#C678DD;--shiki-dark:#C678DD;">local</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. 独立运行模式：standalone</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">3</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. YARN运行模式</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">简单了解：</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. MESOS</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. </span><span style="color:#C678DD;--shiki-dark:#C678DD;">Windows</span></span></code></pre></div><h3 id="_3-2-本地运行模式" tabindex="-1"><a class="header-anchor" href="#_3-2-本地运行模式"><span>3.2 本地运行模式</span></a></h3><h4 id="_3-2-1-本地模式介绍" tabindex="-1"><a class="header-anchor" href="#_3-2-1-本地模式介绍"><span>3.2.1 本地模式介绍</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 1. 什么是本地模式</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> 不需要其他任何节点资源就可以在本地执行Spark代码的环境.</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> 我们之前在IDEA中运行的环境我们称之为开发环境，和本地环境还是不一样</span></span></code></pre></div><h4 id="_3-2-2-解压缩文件" tabindex="-1"><a class="header-anchor" href="#_3-2-2-解压缩文件"><span>3.2.2 解压缩文件</span></a></h4><ul><li>第一步：解压缩</li></ul><ol><li>将spark-2.4.5-bin-without-hadoop-scala-2.12.tgz文件上传到Linux中/opt/software，并解压缩到/opt/module文件目录下，并修改名称为：spark-local</li></ol><div class="language-scala" data-ext="scala" data-title="scala"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//解压</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">tar </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">zxvf spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.4.5</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">bin</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">without</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">scala</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.tgz </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">C</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> /</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">opt</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">/</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">module</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//改名</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">mv spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.4.5</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">bin</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">without</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">scala</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">local</span></span></code></pre></div><ul><li>第二步：Spark关联hadoop，spark2.4.5默认不支持Hadoop3，可以采用多种不同的方式关联Hadoop3</li></ul><ol><li>方法1：修改spark-local/conf/spark-env.sh文件，增加如下内容</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">SPARK_DIST_CLASSPATH</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$(/opt/module/hadoop3/bin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">hadoop</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> classpath)</span></span></code></pre></div><ol start="2"><li>方法2：将hadoop3的jar包上传到==/opt/module/spark-local/jars==</li></ol><h4 id="_3-2-3-启动local环境" tabindex="-1"><a class="header-anchor" href="#_3-2-3-启动local环境"><span>3.2.3 启动local环境</span></a></h4><ol><li>进入/opt/module/spark-local路径，执行如下指令</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-local]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ bin/spark-shell </span><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master local</span></span></code></pre></div><p>![image-20200602192241228](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602192241.png)</p><ol start="2"><li>启动成功后，可以输入网址进行Web UI监控页面访问</li></ol><div class="language-" data-ext="" data-title=""><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span>http://hadoop105:4040</span></span></code></pre></div><p>![image-20200602192318112](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602192318.png)</p><p>3.2.4 命令行工具</p><ol><li>在==<strong>/opt/module/spark-local/data</strong>==目录下创建一个文件：word.txt，在文件中随机写入一些单词</li><li>在==<strong>/opt/module/spark-local</strong>==下，执行如下命令</li></ol><div class="language-scala" data-ext="scala" data-title="scala"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">[atguigu@hadoop105 spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">local]$ sc.textFile(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;data/word.txt&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">).flatMap(_.split(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot; &quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)).map((_,</span><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)).reduceByKey(_</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">+</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">_).collect</span></span></code></pre></div><p>![image-20200602192716232](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602192716.png)</p><h4 id="_3-2-4-退出本地模式" tabindex="-1"><a class="header-anchor" href="#_3-2-4-退出本地模式"><span>3.2.4 退出本地模式</span></a></h4><ul><li>按键Ctrl+C或输入Scala指令</li></ul><div class="language-" data-ext="" data-title=""><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span>：quit</span></span></code></pre></div><h4 id="_3-2-5-提交应用" tabindex="-1"><a class="header-anchor" href="#_3-2-5-提交应用"><span>3.2.5 提交应用</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-local]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ bin/spark-submit \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class org.apache.spark.examples.SparkPi \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master local[2] \\</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">./examples/jars/spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">examples_2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">5</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.jar \\</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span></span></code></pre></div><ul><li>参数说明</li></ul><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">spark-submit ： 提交应用程序</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class 表示要执行程序的主类</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master local[2] ： 本地部署模式，local，本地，[2],数字2表示分配的cpu核数</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">examples_2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">5</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.jar ： 执行程序的主类所在的jar包</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> ： 用于设定当前应用的任务数量</span></span></code></pre></div><p>![image-20200602193202267](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602193202.png)</p><h3 id="_3-3-独立运行模式" tabindex="-1"><a class="header-anchor" href="#_3-3-独立运行模式"><span>3.3 独立运行模式</span></a></h3><h4 id="_3-3-1-独立运行介绍" tabindex="-1"><a class="header-anchor" href="#_3-3-1-独立运行介绍"><span>3.3.1 独立运行介绍</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 什么是独立运行模式</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">   1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. standalone，表示资源调度器和任务调度均是使用Sqark自身的集群来运行。</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">      a、资源调度：</span><span style="color:#C678DD;--shiki-dark:#C678DD;">master</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(Spark的调度者),worker(Spark节点)</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">      b、任务调度：driver(驱动器)，exerutor(执行器)</span></span></code></pre></div><h4 id="_3-3-2-解压缩文件" tabindex="-1"><a class="header-anchor" href="#_3-3-2-解压缩文件"><span>3.3.2 解压缩文件</span></a></h4><ul><li>第一步：解压缩</li></ul><ol><li>将spark-2.4.5-bin-without-hadoop-scala-2.12.tgz文件上传到Linux中/opt/software，并解压缩到/opt/module文件目录下，并修改名称为：spark-standalone</li></ol><div class="language-scala" data-ext="scala" data-title="scala"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//解压</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">tar </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">zxvf spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.4.5</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">bin</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">without</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">scala</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.tgz </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">C</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> /</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">opt</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">/</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">module</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//改名</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">mv spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.4.5</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">bin</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">without</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">scala</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">standalone</span></span></code></pre></div><ul><li>第二步：Spark关联hadoop，spark2.4.5默认不支持Hadoop3，可以采用多种不同的方式关联Hadoop3</li></ul><ol><li>方法1：修改spark-local/conf/spark-env.sh文件，增加如下内容</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">SPARK_DIST_CLASSPATH</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$(/opt/module/hadoop3/bin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">hadoop</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> classpath)</span></span></code></pre></div><ol start="2"><li>方法2：将hadoop3的jar包上传到==/opt/module/spark-local/jars==</li></ol><h4 id="_3-3-3-修改配置文件" tabindex="-1"><a class="header-anchor" href="#_3-3-3-修改配置文件"><span>3.3.3 修改配置文件</span></a></h4><ol><li>进入/opt/module/spark-standalone/conf，修改slaves.template文件名为slaves</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 conf]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ mv </span><span style="color:#D19A66;--shiki-dark:#D19A66;">slaves</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">template</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> slaves</span></span></code></pre></div><ol start="2"><li>修改slaves文件，添加work节点</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop105</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop106</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop107</span></span></code></pre></div><ol start="3"><li><a href="http://xn--spark-env-z89nz78p.sh.xn--templatespark-env-rn60ab6huy0bm27f.sh" target="_blank" rel="noopener noreferrer">修改spark-env.sh.template文件名为spark-env.sh</a></li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 conf]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ mv spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">env</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.template spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">env</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><ol start="4"><li>修改spark-env.sh文件，添加JAVA_HOME环境变量和集群对应的master节点</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">export JAVA_HOME</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/opt/module/</span><span style="color:#D19A66;--shiki-dark:#D19A66;">jdk1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">8</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.0_212</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">SPARK_MASTER_HOST</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop105</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">SPARK_MASTER_PORT</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#D19A66;--shiki-dark:#D19A66;">7077</span></span></code></pre></div><blockquote><p>注意：7077端口，相当于hadoop3内部通信的8020端口</p></blockquote><ol start="5"><li>分发spark-standalone目录</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">xsync spark-standalone</span></span></code></pre></div><h4 id="_3-3-4-启动集群" tabindex="-1"><a class="header-anchor" href="#_3-3-4-启动集群"><span>3.3.4 启动集群</span></a></h4><ol><li>执行脚本命令：</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 打开集群,先启动Master，再启动worker</span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">all</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 关闭集群,先关闭worker，再关闭Master</span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">stop</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">all</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><ol start="2"><li>查看三台服务器的进程</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ myjps</span></span></code></pre></div><p>![image-20200602195125350](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602195125.png)</p><ol start="3"><li>查看Master的资源监控Web UI 网页界面：<a href="http://hadoop105:8080" target="_blank" rel="noopener noreferrer">http://hadoop105:8080</a></li></ol><p>![image-20200602195414292](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602195414.png)</p><h4 id="_3-3-5-提交应用" tabindex="-1"><a class="header-anchor" href="#_3-3-5-提交应用"><span>3.3.5 提交应用</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$bin/spark-submit \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class org.apache.spark.examples.SparkPi \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master spark://hadoop105:7077 \\</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">./examples/jars/spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">examples_2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">5</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.jar \\</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span></span></code></pre></div><ul><li>参数说明</li></ul><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">spark-submit  </span><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 提交应用</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class       -- 表示要执行程序的主类</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master spark://hadoop105:7077 -- 独立运行模式，7070为spark内部通信的端口</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">examples_2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">5</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.jar   </span><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 程序主类所在的jar</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">  -- 设定当前应用的任务数量</span></span></code></pre></div><h4 id="_3-3-6-提交参数说明" tabindex="-1"><a class="header-anchor" href="#_3-3-6-提交参数说明"><span>3.3.6 提交参数说明</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">bin/spark-submit \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class &lt;main-class&gt;</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master &lt;master-url&gt; \\</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">... # other options</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#C678DD;--shiki-dark:#C678DD;">application</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-jar</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> \\</span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[application-arguments]</span></span></code></pre></div><table><thead><tr><th>参数</th><th>解释[可选值举例 ]</th></tr></thead><tbody><tr><td>--class</td><td>Spark程序中包含主函数的类</td></tr><tr><td>--master</td><td>Spark程序运行的模式 <strong>[本地模式：local[*]、spark://linux1:7077、Yarn ]</strong></td></tr><tr><td>--executor-memory 1G</td><td>指定每个executor可用内存为1G <strong>[符合集群内存配置即可，具体情况具体分析 ]</strong></td></tr><tr><td>--total-executor-cores 2</td><td>指定所有executor使用的cpu核数为2个</td></tr><tr><td>--executor-cores</td><td>指定每个executor使用的cpu核数</td></tr><tr><td>application-jar</td><td>打包好的应用jar，包含依赖。这个URL在集群中全局可见。 比如hdfs:// 共享存储系统，如果是file:// path，那么所有的节点的path都包含同样的jar</td></tr><tr><td>application-arguments</td><td>传给main()方法的参数</td></tr></tbody></table><h4 id="_3-3-7-配置历史服务器" tabindex="-1"><a class="header-anchor" href="#_3-3-7-配置历史服务器"><span>3.3.7 配置历史服务器</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 由于spark-shell停止掉后，集群监控hadoop105:4040页面就看不到历史任务的运行情况，所以开发时都配置历史服务器记录任务运行情况</span></span></code></pre></div><ol><li>修改spark-defaults.conf.template文件名为spark-defaults.conf</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 conf]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ mv spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">defaults</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">conf</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.template spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">defaults</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">conf</span></span></code></pre></div><ol start="2"><li>修改spark-default.conf文件，配置日志存储路径</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">spark</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">eventLog</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#C678DD;--shiki-dark:#C678DD;">enabled</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">          true</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">spark</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">eventLog</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.dir               hdfs://hadoop105:</span><span style="color:#D19A66;--shiki-dark:#D19A66;">8020</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/directory</span></span></code></pre></div><blockquote><p>注意：需要启动hadoop集群，HDFS上的directory目录需要提前存在</p></blockquote><ol start="3"><li>修改spark-env.sh文件, 添加日志配置</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">export SPARK_HISTORY_OPTS</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;</span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.history.ui.port=18080 </span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.history.fs.logDirectory=hdfs://hadoop105:8020/directory </span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.history.retainedApplications=30&quot;</span></span></code></pre></div><ul><li><p>参数1含义：WEBUI访问的端口号为18080</p></li><li><p>参数2含义：指定历史服务器日志存储路径</p></li><li><p>参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</p></li></ul><ol start="4"><li>分发配置文件</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">xsync conf</span></span></code></pre></div><ol start="5"><li>重新启动集群和历史服务</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">all</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> </span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-history-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">server</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><ol start="6"><li>重新执行任务</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$bin/spark-submit \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class org.apache.spark.examples.SparkPi \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master spark://hadoop105:7077 \\</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">./examples/jars/spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">examples_2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">5</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.jar \\</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span></span></code></pre></div><ol start="7"><li>查看历史服务：<a href="http://hadoop105:18080" target="_blank" rel="noopener noreferrer">http://hadoop105:18080</a></li></ol><p>![image-20200602205819323](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602205820.png)</p><h4 id="_3-3-8-配置高可用" tabindex="-1"><a class="header-anchor" href="#_3-3-8-配置高可用"><span>3.3.8 配置高可用</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 因为Spark集群中的Master只有一个，存在单点故障，所以需要给集群配置多个Master，一旦处于活跃的Master故障时，StandBy的Master转换为活跃的，提供服务。</span></span></code></pre></div><ul><li>集群规划</li></ul><table><thead><tr><th style="text-align:center;"></th><th style="text-align:center;">Hadoop105</th><th style="text-align:center;">Hadoop106</th><th style="text-align:center;">Hadoop107</th></tr></thead><tbody><tr><td style="text-align:center;">Spark</td><td style="text-align:center;">MasterZookeeperWorker</td><td style="text-align:center;">MasterZookeeperWorker</td><td style="text-align:center;">ZookeeperWorker</td></tr></tbody></table><ol><li>停止集群</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">stop</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">all</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><ol start="2"><li>启动Zookeeper</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ zk </span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span></span></code></pre></div><ol start="3"><li>修改spark-env.sh文件添加如下配置</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">#注释如下内容：</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">#SPARK_MASTER_HOST</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">linux1</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">#SPARK_MASTER_PORT</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#D19A66;--shiki-dark:#D19A66;">7077</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">SPARK_MASTER_WEBUI_PORT</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#D19A66;--shiki-dark:#D19A66;">8989</span></span>
<span class="line"></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">#添加如下内容:</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">export SPARK_DAEMON_JAVA_OPTS</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;</span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.deploy.recoveryMode=ZOOKEEPER </span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.deploy.zookeeper.url=hadoop105,hadoop106,hadoop107 </span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.deploy.zookeeper.dir=/spark&quot;</span></span></code></pre></div><ol start="4"><li>分发配置文件</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ xsync conf/</span></span></code></pre></div><ol start="5"><li>启动集群</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">all</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><ol start="6"><li>启动lhadoop106的单独Master节点，此时hadoop106节点Master状态处于备用状态</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop106 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">master</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><p>![image-20200602211112909](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602211114.png)</p><ol start="7"><li>停止hadoop105的Master资源监控进程</li><li>查看hadoop106的Master 资源监控Web UI，稍等一段时间后，hadoop106节点的Master状态提升为活动状态</li></ol><p>![image-20200602211320022](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602211320.png)</p><h3 id="_3-4-yarn" tabindex="-1"><a class="header-anchor" href="#_3-4-yarn"><span>3.4 YARN</span></a></h3><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. Spark重点在与实时的计算引擎，而不是资源框架，所以本身提供资源调度并不是其本身。</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. 所以参考在YARN环境下运行Spark。</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">3</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">. 资源调度：hadoop的yarn</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">   执行任务： spark的driver和executor</span></span></code></pre></div><h4 id="_3-4-1-解压缩文件" tabindex="-1"><a class="header-anchor" href="#_3-4-1-解压缩文件"><span>3.4.1 解压缩文件</span></a></h4><ul><li>第一步：解压缩</li></ul><ol><li>将spark-2.4.5-bin-without-hadoop-scala-2.12.tgz文件上传到Linux中/opt/software，并解压缩到/opt/module文件目录下，并修改名称为：spark-yarn</li></ol><div class="language-scala" data-ext="scala" data-title="scala"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//解压</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">tar </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">zxvf spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.4.5</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">bin</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">without</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">scala</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.tgz </span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#E5C07B;--shiki-dark:#E5C07B;">C</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;"> /</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">opt</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">/</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">module</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">//改名</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">mv spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.4.5</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">bin</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">without</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">scala</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2.12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> spark</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">-</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">yarn</span></span></code></pre></div><ul><li>第二步：Spark关联hadoop，spark2.4.5默认不支持Hadoop3，可以采用多种不同的方式关联Hadoop3</li></ul><ol><li>方法1：修改spark-local/conf/spark-env.sh文件，增加如下内容</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">SPARK_DIST_CLASSPATH</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$(/opt/module/hadoop3/bin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">hadoop</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;"> classpath)</span></span></code></pre></div><ol start="2"><li>方法2：将hadoop3的jar包上传到==/opt/module/spark-local/jars==</li></ol><h4 id="_3-4-2-修改配置文件" tabindex="-1"><a class="header-anchor" href="#_3-4-2-修改配置文件"><span>3.4.2 修改配置文件</span></a></h4><ol><li>修改hadoop配置文件/opt/module/hadoop/etc/hadoop/yarn-site.xml, <mark>并分发</mark></li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">!</span><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">property</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">     &lt;</span><span style="color:#C678DD;--shiki-dark:#C678DD;">name</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span><span style="color:#D19A66;--shiki-dark:#D19A66;">yarn</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">nodemanager</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.pmem-</span><span style="color:#C678DD;--shiki-dark:#C678DD;">check</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#C678DD;--shiki-dark:#C678DD;">enabled</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">name</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">     &lt;</span><span style="color:#C678DD;--shiki-dark:#C678DD;">value</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">false</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">value</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/property</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span>
<span class="line"></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">!</span><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">property</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">     &lt;</span><span style="color:#C678DD;--shiki-dark:#C678DD;">name</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span><span style="color:#D19A66;--shiki-dark:#D19A66;">yarn</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">nodemanager</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.vmem-</span><span style="color:#C678DD;--shiki-dark:#C678DD;">check</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#C678DD;--shiki-dark:#C678DD;">enabled</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">name</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">     &lt;</span><span style="color:#C678DD;--shiki-dark:#C678DD;">value</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">false</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">value</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span>
<span class="line"><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&lt;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/property</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">&gt;</span></span></code></pre></div><ol start="2"><li>修改conf/spark-env.sh，添加JAVA_HOME和YARN_CONF_DIR配置</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 改名</span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 conf]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ mv spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">env</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.template spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">env</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span>
<span class="line"></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">-- 添加配置</span></span>
<span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 conf]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ vim spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">env</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">export JAVA_HOME</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/opt/module/</span><span style="color:#D19A66;--shiki-dark:#D19A66;">jdk1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">8</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.0_212</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">YARN_CONF_DIR</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/opt/module/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">hadoop</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">3</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">3</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/etc/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">hadoop</span></span></code></pre></div><h4 id="_3-4-3-启动hadoop集群" tabindex="-1"><a class="header-anchor" href="#_3-4-3-启动hadoop集群"><span>3.4.3 启动hadoop集群</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 conf]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ mycluster </span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span></span></code></pre></div><h4 id="_3-4-4-提交应用" tabindex="-1"><a class="header-anchor" href="#_3-4-4-提交应用"><span>3.4.4 提交应用</span></a></h4><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$bin/spark-submit \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class org.apache.spark.examples.SparkPi \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master spark://hadoop105:7077 \\</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">./examples/jars/spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">examples_2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">5</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.jar \\</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span></span></code></pre></div><p>查看http://hadoop105:8088页面，点击History，查看历史页面</p><h4 id="_3-4-5-配置历史服务器" tabindex="-1"><a class="header-anchor" href="#_3-4-5-配置历史服务器"><span>3.4.5 配置历史服务器</span></a></h4><ol><li>修改spark-defaults.conf.template文件名为spark-defaults.conf</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 conf]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$ mv spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">defaults</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">conf</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.template spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">defaults</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">conf</span></span></code></pre></div><ol start="2"><li>修改spark-default.conf文件，配置日志存储路径</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">spark</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">eventLog</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#C678DD;--shiki-dark:#C678DD;">enabled</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">          true</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">spark</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">eventLog</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.dir               hdfs://hadoop105:</span><span style="color:#D19A66;--shiki-dark:#D19A66;">8020</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">/directory</span></span></code></pre></div><blockquote><p>注意：需要启动hadoop集群，HDFS上的directory目录需要提前存在</p></blockquote><ol start="3"><li>修改spark-env.sh文件, 添加日志配置</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">export SPARK_HISTORY_OPTS</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;</span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.history.ui.port=18080 </span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.history.fs.logDirectory=hdfs://hadoop105:8020/directory </span></span>
<span class="line"><span style="color:#98C379;--shiki-dark:#98C379;">-Dspark.history.retainedApplications=30&quot;</span></span></code></pre></div><ul><li><p>参数1含义：WEBUI访问的端口号为18080</p></li><li><p>参数2含义：指定历史服务器日志存储路径</p></li><li><p>参数3含义：指定保存Application历史记录的个数，如果超过这个值，旧的应用程序信息将被删除，这个是内存中的应用数，而不是页面上显示的应用数。</p></li></ul><ol start="4"><li>修改spark-defaults.conf</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">spark</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">yarn</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">historyServer</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">address</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">hadoop105:</span><span style="color:#D19A66;--shiki-dark:#D19A66;">18080</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">spark</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">history</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">ui</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">port</span><span style="color:#56B6C2;--shiki-dark:#56B6C2;">=</span><span style="color:#D19A66;--shiki-dark:#D19A66;">18080</span></span></code></pre></div><ol start="5"><li>启动历史服务</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">sbin/</span><span style="color:#C678DD;--shiki-dark:#C678DD;">start</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-history-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">server</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">sh</span></span></code></pre></div><ol start="6"><li>重新提交应用</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#E06C75;--shiki-dark:#E06C75;">[atguigu@hadoop105 spark-standalone]</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">$bin/spark-submit \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--class org.apache.spark.examples.SparkPi \\</span></span>
<span class="line"><span style="color:#7F848E;font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;">--master spark://hadoop105:7077 \\</span></span>
<span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">./examples/jars/spark-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">examples_2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">12</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">-</span><span style="color:#D19A66;--shiki-dark:#D19A66;">2</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">4</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">5</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.jar \\</span></span>
<span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">10</span></span></code></pre></div><p>![img](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602215038.jpg)</p><ol start="7"><li>Web页面查看日志：<a href="http://hadoop106:8088" target="_blank" rel="noopener noreferrer">http://hadoop106:8088</a></li></ol><h3 id="_3-5-k8s-mesos模式" tabindex="-1"><a class="header-anchor" href="#_3-5-k8s-mesos模式"><span>3.5 K8S &amp; Mesos模式</span></a></h3><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">Mesos是Apache下的开源分布式资源管理框架，它被称为是分布式系统的内核,在Twitter得到广泛使用,管理着Twitter超过30,0000台服务器上的应用部署，但是在国内，依然使用着传统的Hadoop大数据框架，所以国内使用Mesos框架的并不多，但是原理其实都差不多，这里我们就不做过多讲解了。</span></span></code></pre></div><p>![image-20200602215334165](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602215334.png)</p><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">容器化部署是目前业界很流行的一项技术，基于Docker镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是Kubernetes（k8s），而Spark也在最近的版本中支持了k8s部署模式</span></span></code></pre></div><p>![image-20200602215352244](<a href="https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic" target="_blank" rel="noopener noreferrer">https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic</a> GO/20200602215352.png)</p><h3 id="_3-6-windows-模式" tabindex="-1"><a class="header-anchor" href="#_3-6-windows-模式"><span>3.6 Windows 模式</span></a></h3><ol><li>将文件spark-2.4.5-bin-without-hadoop-scala-2.12.tgz解压缩到无中文无空格的路径中，将hadoop3依赖jar包拷贝到jars目录中</li><li>执行解压缩文件路径下bin目录中的spark-shell.cmd文件，启动Spark本地环境</li><li>在bin目录中创建input目录，并添加word.txt文件, 在命令行中输入脚本代码</li></ol><div class="language-sql" data-ext="sql" data-title="sql"><pre class="shiki shiki-themes one-dark-pro one-dark-pro vp-code" style="background-color:#282c34;--shiki-dark-bg:#282c34;color:#abb2bf;--shiki-dark:#abb2bf;" tabindex="0"><code><span class="line"><span style="color:#D19A66;--shiki-dark:#D19A66;">sc</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">textFile</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot;input&quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">).flatMap(</span><span style="color:#D19A66;--shiki-dark:#D19A66;">_</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">.</span><span style="color:#D19A66;--shiki-dark:#D19A66;">split</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">(</span><span style="color:#98C379;--shiki-dark:#98C379;">&quot; &quot;</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)).map((_,</span><span style="color:#D19A66;--shiki-dark:#D19A66;">1</span><span style="color:#ABB2BF;--shiki-dark:#ABB2BF;">)).reduceByKey(_+_).collect</span></span></code></pre></div><h3 id="_3-7-部署模式的对比" tabindex="-1"><a class="header-anchor" href="#_3-7-部署模式的对比"><span>3.7 部署模式的对比</span></a></h3><table><thead><tr><th style="text-align:center;">模式</th><th style="text-align:center;">Spark安装机器数</th><th style="text-align:center;">需启动的进程</th><th style="text-align:center;">所属者</th><th style="text-align:center;">应用场景</th></tr></thead><tbody><tr><td style="text-align:center;">Local</td><td style="text-align:center;">1</td><td style="text-align:center;">无</td><td style="text-align:center;">Spark</td><td style="text-align:center;">测试</td></tr><tr><td style="text-align:center;">Standalone</td><td style="text-align:center;">3</td><td style="text-align:center;">Master及Worker</td><td style="text-align:center;">Spark</td><td style="text-align:center;">单独部署</td></tr><tr><td style="text-align:center;">Yarn</td><td style="text-align:center;">1</td><td style="text-align:center;">Yarn及HDFS</td><td style="text-align:center;">Hadoop</td><td style="text-align:center;">混合部署</td></tr></tbody></table><h3 id="_3-8-端口号" tabindex="-1"><a class="header-anchor" href="#_3-8-端口号"><span>3.8 端口号</span></a></h3><table><thead><tr><th>端口</th><th>作用</th></tr></thead><tbody><tr><td>8080</td><td>资源的监控页面端口</td></tr><tr><td>7077</td><td>Spark的worker内部通信的端口</td></tr><tr><td>4040</td><td>计算的监控页面端口</td></tr><tr><td>18080</td><td>历史服务器端口</td></tr><tr><td>8088</td><td>RM的资源监控端口</td></tr></tbody></table>`,203)]))}const t=a(o,[["render",r],["__file","1.Spark环境的安装.html.vue"]]),k=JSON.parse('{"path":"/posts/BigData/08_Spark/1.Spark%E7%8E%AF%E5%A2%83%E7%9A%84%E5%AE%89%E8%A3%85.html","title":"Spark环境的安装","lang":"zh-CN","frontmatter":{"description":"Spark环境的安装 一、 Spark简介 1.1 Spark是什么 是一种基于内存的快速、通用、可拓展的大数据分析计算引擎。 1.2 Hadoop 和 Spark关联 hadoop ：2013年10月发布2.X (Yarn)版本； spark ： 2013年6月，Spark成为了Apache基金会下的项目。 Spark可以理解为hadoop MR的升...","head":[["meta",{"property":"og:url","content":"https://springg.us.kg/posts/BigData/08_Spark/1.Spark%E7%8E%AF%E5%A2%83%E7%9A%84%E5%AE%89%E8%A3%85.html"}],["meta",{"property":"og:site_name","content":"mrjason’s Blog"}],["meta",{"property":"og:title","content":"Spark环境的安装"}],["meta",{"property":"og:description","content":"Spark环境的安装 一、 Spark简介 1.1 Spark是什么 是一种基于内存的快速、通用、可拓展的大数据分析计算引擎。 1.2 Hadoop 和 Spark关联 hadoop ：2013年10月发布2.X (Yarn)版本； spark ： 2013年6月，Spark成为了Apache基金会下的项目。 Spark可以理解为hadoop MR的升..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602020855.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-10-28T01:58:08.000Z"}],["meta",{"property":"article:author","content":"MrJason"}],["meta",{"property":"article:modified_time","content":"2024-10-28T01:58:08.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Spark环境的安装\\",\\"image\\":[\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602020855.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602021032.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602021626.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602021747.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602022304.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602022554.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602181641.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602185736.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602192241.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602192318.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602192716.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602193202.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602195125.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602195414.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602205820.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602211114.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602211320.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602215038.jpg\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602215334.png\\",\\"https://lian-zp.oss-cn-shenzhen.aliyuncs.com/pic GO/20200602215352.png\\"],\\"dateModified\\":\\"2024-10-28T01:58:08.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"MrJason\\",\\"url\\":\\"https://springg.us.kg\\"}]}"]]},"headers":[{"level":2,"title":"一、 Spark简介","slug":"一、-spark简介","link":"#一、-spark简介","children":[{"level":3,"title":"1.1 Spark是什么","slug":"_1-1-spark是什么","link":"#_1-1-spark是什么","children":[]},{"level":3,"title":"1.2 Hadoop 和 Spark关联","slug":"_1-2-hadoop-和-spark关联","link":"#_1-2-hadoop-和-spark关联","children":[]},{"level":3,"title":"1.3 Spark的核心框架","slug":"_1-3-spark的核心框架","link":"#_1-3-spark的核心框架","children":[]}]},{"level":2,"title":"二、Spark快速上手","slug":"二、spark快速上手","link":"#二、spark快速上手","children":[{"level":3,"title":"创建Maven工程","slug":"创建maven工程","link":"#创建maven工程","children":[]},{"level":3,"title":"2.1 增加Scala插件","slug":"_2-1-增加scala插件","link":"#_2-1-增加scala插件","children":[]},{"level":3,"title":"2.2  增加依赖关系","slug":"_2-2-增加依赖关系","link":"#_2-2-增加依赖关系","children":[]},{"level":3,"title":"2.3  增加日志文件配置文件","slug":"_2-3-增加日志文件配置文件","link":"#_2-3-增加日志文件配置文件","children":[]},{"level":3,"title":"2.4   WordCount","slug":"_2-4-wordcount","link":"#_2-4-wordcount","children":[]}]},{"level":2,"title":"三、 Spark运行环境","slug":"三、-spark运行环境","link":"#三、-spark运行环境","children":[{"level":3,"title":"3.1 Spark的运行环境","slug":"_3-1-spark的运行环境","link":"#_3-1-spark的运行环境","children":[]},{"level":3,"title":"3.2 本地运行模式","slug":"_3-2-本地运行模式","link":"#_3-2-本地运行模式","children":[]},{"level":3,"title":"3.3 独立运行模式","slug":"_3-3-独立运行模式","link":"#_3-3-独立运行模式","children":[]},{"level":3,"title":"3.4 YARN","slug":"_3-4-yarn","link":"#_3-4-yarn","children":[]},{"level":3,"title":"3.5 K8S & Mesos模式","slug":"_3-5-k8s-mesos模式","link":"#_3-5-k8s-mesos模式","children":[]},{"level":3,"title":"3.6 Windows 模式","slug":"_3-6-windows-模式","link":"#_3-6-windows-模式","children":[]},{"level":3,"title":"3.7 部署模式的对比","slug":"_3-7-部署模式的对比","link":"#_3-7-部署模式的对比","children":[]},{"level":3,"title":"3.8 端口号","slug":"_3-8-端口号","link":"#_3-8-端口号","children":[]}]}],"git":{"createdTime":1730080688000,"updatedTime":1730080688000,"contributors":[{"name":"MrJason","email":"845886914@qq.com","commits":1}]},"readingTime":{"minutes":14.41,"words":4324},"filePathRelative":"posts/BigData/08_Spark/1.Spark环境的安装.md","localizedDate":"2024年10月28日","excerpt":"\\n<hr>\\n<h2>一、 Spark简介</h2>\\n<h3>1.1 Spark是什么</h3>\\n<blockquote>\\n<p>是一种基于<strong>内存</strong>的快速、通用、可拓展的大数据<strong>分析计算引擎</strong>。</p>\\n</blockquote>\\n<h3>1.2 Hadoop 和 Spark关联</h3>\\n<blockquote>\\n<ol>\\n<li>hadoop ：2013年10月发布2.X (Yarn)版本；</li>\\n<li>spark ：   2013年6月，Spark成为了Apache基金会下的项目。</li>\\n<li>Spark可以理解为hadoop MR的升级版。</li>\\n</ol>\\n</blockquote>","autoDesc":true}');export{t as comp,k as data};
